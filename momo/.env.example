# =============================================================================
# Momo Configuration
# =============================================================================
# Copy this file to .env and modify as needed.
# All values shown are defaults - you only need to set what you want to change.
# =============================================================================

# -----------------------------------------------------------------------------
# Server Configuration
# -----------------------------------------------------------------------------
MOMO_HOST=0.0.0.0
MOMO_PORT=3000
# Runtime mode: all (API + workers), api (HTTP only), worker (background jobs only)
MOMO_RUNTIME_MODE=all
# Comma-separated API keys for authentication. Leave empty for no auth.
# MOMO_API_KEYS=key1,key2,key3

# -----------------------------------------------------------------------------
# MCP Configuration (Built-in Model Context Protocol Server)
# -----------------------------------------------------------------------------
# Enable the in-process MCP server.
MOMO_MCP_ENABLED=true
# Path where MCP streamable HTTP endpoint is mounted.
MOMO_MCP_PATH=/mcp
# Require Bearer auth on MCP requests using MOMO_API_KEYS.
MOMO_MCP_REQUIRE_AUTH=true
# Default container/project tag when none is provided by MCP clients.
MOMO_MCP_DEFAULT_CONTAINER_TAG=default
# Header used for project scoping (Supermemory-compatible by default).
MOMO_MCP_PROJECT_HEADER=x-sm-project
# Optional externally reachable base URL used in OAuth discovery responses.
# MOMO_MCP_PUBLIC_URL=https://mcp.example.com
# Optional OAuth authorization server issuer URL for discovery metadata.
# MOMO_MCP_AUTHORIZATION_SERVER=https://auth.example.com

# -----------------------------------------------------------------------------
# Database Configuration
# -----------------------------------------------------------------------------
# Local SQLite file (default)
DATABASE_URL=file:momo.db

# Turso/LibSQL remote database
# DATABASE_URL=libsql://your-db.turso.io
# DATABASE_AUTH_TOKEN=your-auth-token

# Local replica for remote database (optional)
# DATABASE_LOCAL_PATH=local-replica.db

# Optional dedicated read path (replica strategy)
# If set, search reads use this backend while writes continue to use DATABASE_URL.
# DATABASE_READ_URL=libsql://your-read-replica.turso.io
# DATABASE_READ_AUTH_TOKEN=your-read-auth-token
# DATABASE_READ_LOCAL_PATH=local-read-replica.db
DATABASE_READ_SYNC_INTERVAL_SECS=2

# SQLite/LibSQL lock-tuning (local databases)
# Wait this long when database is busy before returning "database is locked"
DATABASE_BUSY_TIMEOUT_MS=5000
# Journal mode for local SQLite files (WAL allows readers during writes)
DATABASE_JOURNAL_MODE=WAL
# Durability/perf tradeoff (NORMAL is a good default for WAL)
DATABASE_SYNCHRONOUS=NORMAL
# Number of chunk rows written per transaction batch during ingestion
DATABASE_WRITE_BATCH_SIZE=128
# Optional pause between write batches to improve query tail-latency under heavy ingest
DATABASE_WRITE_BATCH_PAUSE_MS=0

# -----------------------------------------------------------------------------
# Embedding Configuration
# -----------------------------------------------------------------------------
# Local embedding model (FastEmbed - no API key needed)
EMBEDDING_MODEL=BAAI/bge-small-en-v1.5
EMBEDDING_DIMENSIONS=384
EMBEDDING_BATCH_SIZE=256
# Max passages per embedding call during ingestion (smaller => better query interleaving)
EMBEDDING_INGEST_BATCH_SIZE=32
# Use separate local model instances for query and ingestion embeddings
EMBEDDING_DUAL_MODEL=true
# Optional pause between ingestion embedding batches
EMBEDDING_INGEST_BATCH_PAUSE_MS=0

# External embedding API (OpenAI, OpenRouter, Ollama, etc.)
# Format: provider/model-name
# EMBEDDING_MODEL=openai/text-embedding-3-small
# EMBEDDING_MODEL=openrouter/openai/text-embedding-3-small
# EMBEDDING_MODEL=ollama/nomic-embed-text

# API configuration for external providers
# EMBEDDING_API_KEY=sk-your-api-key
# EMBEDDING_BASE_URL=https://api.openai.com/v1
# EMBEDDING_RATE_LIMIT=10
EMBEDDING_TIMEOUT=30
EMBEDDING_MAX_RETRIES=3

# -----------------------------------------------------------------------------
# Processing Configuration
# -----------------------------------------------------------------------------
# Chunk size in tokens
CHUNK_SIZE=512
# Overlap between chunks in tokens
CHUNK_OVERLAP=50
# Maximum content length in bytes (default: 10MB)
MAX_CONTENT_LENGTH=10000000
# Worker polling interval for queued/processing documents
PROCESSING_POLL_INTERVAL_SECS=10

# -----------------------------------------------------------------------------
# OCR Configuration (Image Text Extraction)
# -----------------------------------------------------------------------------
# Local OCR with Tesseract (default - requires tesseract installed)
OCR_MODEL=local/tesseract

# Cloud OCR providers
# OCR_MODEL=mistral/pixtral-12b
# OCR_MODEL=openai/gpt-4o

# OCR API configuration (for cloud providers)
# OCR_API_KEY=your-api-key
# OCR_BASE_URL=https://api.example.com/v1

# Tesseract language codes (comma-separated)
# See: https://tesseract-ocr.github.io/tessdoc/Data-Files-in-different-versions.html
OCR_LANGUAGES=eng
OCR_TIMEOUT=60

# Image dimension limits (in pixels)
OCR_MAX_DIMENSION=4096
OCR_MIN_DIMENSION=50

# -----------------------------------------------------------------------------
# Audio Transcription Configuration
# -----------------------------------------------------------------------------
# NOTE: Audio transcription is partially implemented (Phase 2).
# Local transcription requires whisper.cpp integration.

# Local transcription with Whisper (default)
TRANSCRIPTION_MODEL=local/whisper-small

# Cloud transcription providers
# TRANSCRIPTION_MODEL=openai/whisper-1

# Transcription API configuration
# TRANSCRIPTION_API_KEY=your-api-key
# TRANSCRIPTION_BASE_URL=https://api.openai.com/v1

# Path to local whisper model file (optional)
# TRANSCRIPTION_MODEL_PATH=/path/to/whisper-small.bin

# Timeout in seconds (default: 5 minutes)
TRANSCRIPTION_TIMEOUT=300
# Maximum file size in bytes (default: 100MB)
TRANSCRIPTION_MAX_FILE_SIZE=104857600
# Maximum audio duration in seconds (default: 2 hours)
TRANSCRIPTION_MAX_DURATION=7200

# -----------------------------------------------------------------------------
# LLM Configuration (for AI-powered features)
# -----------------------------------------------------------------------------
# NOTE: LLM is optional. If not configured, intelligence features are disabled.
# Phase 4+ features (memory extraction, relationships) require LLM.

# Format: provider/model-name
# LLM_MODEL=openai/gpt-4o-mini
# LLM_MODEL=openai/gpt-4o
# LLM_MODEL=openrouter/anthropic/claude-3-5-sonnet
# LLM_MODEL=ollama/llama3.2

# LLM API configuration
# LLM_API_KEY=sk-your-api-key
# LLM_BASE_URL=https://api.openai.com/v1
LLM_TIMEOUT=30
LLM_MAX_RETRIES=3

# -----------------------------------------------------------------------------
# Logging
# -----------------------------------------------------------------------------
# Log levels: error, warn, info, debug, trace
RUST_LOG=momo=info,tower_http=debug
